{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging using modified Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')], [('Rudolph', 'NOUN'), ('Agnew', 'NOUN'), (',', '.'), ('55', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), ('and', 'CONJ'), ('former', 'ADJ'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Consolidated', 'NOUN'), ('Gold', 'NOUN'), ('Fields', 'NOUN'), ('PLC', 'NOUN'), (',', '.'), ('was', 'VERB'), ('named', 'VERB'), ('*-1', 'X'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('British', 'ADJ'), ('industrial', 'ADJ'), ('conglomerate', 'NOUN'), ('.', '.')], [('A', 'DET'), ('form', 'NOUN'), ('of', 'ADP'), ('asbestos', 'NOUN'), ('once', 'ADV'), ('used', 'VERB'), ('*', 'X'), ('*', 'X'), ('to', 'PRT'), ('make', 'VERB'), ('Kent', 'NOUN'), ('cigarette', 'NOUN'), ('filters', 'NOUN'), ('has', 'VERB'), ('caused', 'VERB'), ('a', 'DET'), ('high', 'ADJ'), ('percentage', 'NOUN'), ('of', 'ADP'), ('cancer', 'NOUN'), ('deaths', 'NOUN'), ('among', 'ADP'), ('a', 'DET'), ('group', 'NOUN'), ('of', 'ADP'), ('workers', 'NOUN'), ('exposed', 'VERB'), ('*', 'X'), ('to', 'PRT'), ('it', 'PRON'), ('more', 'ADV'), ('than', 'ADP'), ('30', 'NUM'), ('years', 'NOUN'), ('ago', 'ADP'), (',', '.'), ('researchers', 'NOUN'), ('reported', 'VERB'), ('0', 'X'), ('*T*-1', 'X'), ('.', '.')], [('The', 'DET'), ('asbestos', 'NOUN'), ('fiber', 'NOUN'), (',', '.'), ('crocidolite', 'NOUN'), (',', '.'), ('is', 'VERB'), ('unusually', 'ADV'), ('resilient', 'ADJ'), ('once', 'ADP'), ('it', 'PRON'), ('enters', 'VERB'), ('the', 'DET'), ('lungs', 'NOUN'), (',', '.'), ('with', 'ADP'), ('even', 'ADV'), ('brief', 'ADJ'), ('exposures', 'NOUN'), ('to', 'PRT'), ('it', 'PRON'), ('causing', 'VERB'), ('symptoms', 'NOUN'), ('that', 'DET'), ('*T*-1', 'X'), ('show', 'VERB'), ('up', 'PRT'), ('decades', 'NOUN'), ('later', 'ADJ'), (',', '.'), ('researchers', 'NOUN'), ('said', 'VERB'), ('0', 'X'), ('*T*-2', 'X'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# first 5 tagged sentences\n",
    "print(nltk_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets split the data into training and validation set in ratio 90:10\n",
    "train_set, valid_set = train_test_split(nltk_data,train_size=0.95,test_size=0.05,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95589\n",
      "5087\n"
     ]
    }
   ],
   "source": [
    "# create list of train tagged words with both words and POS tags \n",
    "# creating the list of words for the valdiation set \n",
    "train_tagged_words = [tup for sent in train_set for tup in sent]\n",
    "valid_words = [tup[0] for sent in valid_set for tup in sent]\n",
    "print(len(train_tagged_words))\n",
    "print(len(valid_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train tagged words\n",
      "[('Bank', 'NOUN'), ('of', 'ADP'), ('New', 'NOUN'), ('England', 'NOUN'), (\"'s\", 'PRT'), ('shares', 'NOUN'), ('are', 'VERB'), ('traded', 'VERB'), ('*-1', 'X'), ('on', 'ADP')]\n"
     ]
    }
   ],
   "source": [
    "# some of the tagged words in train set\n",
    "print(\"train tagged words\")\n",
    "print(train_tagged_words[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the unique tags in the train set\n",
    "unique_train_tags = {tag for word,tag in train_tagged_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique tags in train set is 12\n",
      "unique tags are {'NOUN', 'CONJ', 'X', 'ADJ', 'DET', 'ADP', 'ADV', 'PRT', 'NUM', '.', 'PRON', 'VERB'}\n"
     ]
    }
   ],
   "source": [
    "print(\"number of unique tags in train set is %s\" %(len(unique_train_tags)))\n",
    "print(\"unique tags are %s\" %(unique_train_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vanilla Viterbi based POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate the emission probability\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
    "    count_tag = len(tag_list)\n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "    count_w_given_tag = len(w_given_tag_list)\n",
    "    return (count_w_given_tag, count_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate the transmission probability\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    count_t1 = len([t for t in tags if t==t1])\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1, count_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5302, 8284)\n"
     ]
    }
   ],
   "source": [
    "# testing the function written to get the transmission probability\n",
    "print(t2_given_t1('NOUN','DET'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating t x t transition matrix of tags\n",
    "# each column is t2, each row is t1\n",
    "# thus M(i, j) represents P(tj given ti)\n",
    "\n",
    "tags_matrix = np.zeros((len(unique_train_tags), len(unique_train_tags)), dtype='float32')\n",
    "for i, t1 in enumerate(list(unique_train_tags)):\n",
    "    for j, t2 in enumerate(list(unique_train_tags)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the matrix to a df for better readability\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(unique_train_tags), index=list(unique_train_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOUN</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>X</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>PRT</th>\n",
       "      <th>NUM</th>\n",
       "      <th>.</th>\n",
       "      <th>PRON</th>\n",
       "      <th>VERB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>NOUN</td>\n",
       "      <td>0.264632</td>\n",
       "      <td>0.041936</td>\n",
       "      <td>0.029136</td>\n",
       "      <td>0.012289</td>\n",
       "      <td>0.013310</td>\n",
       "      <td>0.176275</td>\n",
       "      <td>0.016884</td>\n",
       "      <td>0.043832</td>\n",
       "      <td>0.009627</td>\n",
       "      <td>0.239179</td>\n",
       "      <td>0.004923</td>\n",
       "      <td>0.147978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CONJ</td>\n",
       "      <td>0.349132</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.007977</td>\n",
       "      <td>0.116847</td>\n",
       "      <td>0.121539</td>\n",
       "      <td>0.054435</td>\n",
       "      <td>0.055842</td>\n",
       "      <td>0.004693</td>\n",
       "      <td>0.042234</td>\n",
       "      <td>0.034256</td>\n",
       "      <td>0.058658</td>\n",
       "      <td>0.153918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>X</td>\n",
       "      <td>0.061345</td>\n",
       "      <td>0.010357</td>\n",
       "      <td>0.076482</td>\n",
       "      <td>0.016571</td>\n",
       "      <td>0.055131</td>\n",
       "      <td>0.142925</td>\n",
       "      <td>0.025175</td>\n",
       "      <td>0.185787</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>0.163799</td>\n",
       "      <td>0.056087</td>\n",
       "      <td>0.203633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ADJ</td>\n",
       "      <td>0.696725</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021392</td>\n",
       "      <td>0.066645</td>\n",
       "      <td>0.005101</td>\n",
       "      <td>0.078986</td>\n",
       "      <td>0.004608</td>\n",
       "      <td>0.010861</td>\n",
       "      <td>0.020405</td>\n",
       "      <td>0.065328</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.012342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>DET</td>\n",
       "      <td>0.640029</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.045509</td>\n",
       "      <td>0.204973</td>\n",
       "      <td>0.005311</td>\n",
       "      <td>0.009054</td>\n",
       "      <td>0.012313</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.021970</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.003742</td>\n",
       "      <td>0.038387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ADP</td>\n",
       "      <td>0.321776</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.034029</td>\n",
       "      <td>0.105297</td>\n",
       "      <td>0.326378</td>\n",
       "      <td>0.017228</td>\n",
       "      <td>0.013162</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.062921</td>\n",
       "      <td>0.039486</td>\n",
       "      <td>0.069128</td>\n",
       "      <td>0.008240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ADV</td>\n",
       "      <td>0.030897</td>\n",
       "      <td>0.006312</td>\n",
       "      <td>0.023588</td>\n",
       "      <td>0.130233</td>\n",
       "      <td>0.067110</td>\n",
       "      <td>0.119601</td>\n",
       "      <td>0.081063</td>\n",
       "      <td>0.013621</td>\n",
       "      <td>0.030565</td>\n",
       "      <td>0.136877</td>\n",
       "      <td>0.015615</td>\n",
       "      <td>0.344518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>PRT</td>\n",
       "      <td>0.242563</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>0.013403</td>\n",
       "      <td>0.086303</td>\n",
       "      <td>0.100360</td>\n",
       "      <td>0.021576</td>\n",
       "      <td>0.010134</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>0.058516</td>\n",
       "      <td>0.041517</td>\n",
       "      <td>0.018960</td>\n",
       "      <td>0.402746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NUM</td>\n",
       "      <td>0.355338</td>\n",
       "      <td>0.013381</td>\n",
       "      <td>0.206661</td>\n",
       "      <td>0.034196</td>\n",
       "      <td>0.003271</td>\n",
       "      <td>0.034790</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>0.027951</td>\n",
       "      <td>0.184062</td>\n",
       "      <td>0.118347</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.017544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>.</td>\n",
       "      <td>0.223152</td>\n",
       "      <td>0.057924</td>\n",
       "      <td>0.026623</td>\n",
       "      <td>0.044972</td>\n",
       "      <td>0.173502</td>\n",
       "      <td>0.091114</td>\n",
       "      <td>0.052078</td>\n",
       "      <td>0.002339</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.093812</td>\n",
       "      <td>0.065389</td>\n",
       "      <td>0.088505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>PRON</td>\n",
       "      <td>0.211230</td>\n",
       "      <td>0.004966</td>\n",
       "      <td>0.092819</td>\n",
       "      <td>0.074866</td>\n",
       "      <td>0.009549</td>\n",
       "      <td>0.022918</td>\n",
       "      <td>0.033995</td>\n",
       "      <td>0.012223</td>\n",
       "      <td>0.007257</td>\n",
       "      <td>0.041253</td>\n",
       "      <td>0.008021</td>\n",
       "      <td>0.480901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>VERB</td>\n",
       "      <td>0.110904</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>0.218005</td>\n",
       "      <td>0.064649</td>\n",
       "      <td>0.133101</td>\n",
       "      <td>0.090493</td>\n",
       "      <td>0.082577</td>\n",
       "      <td>0.031121</td>\n",
       "      <td>0.022817</td>\n",
       "      <td>0.035312</td>\n",
       "      <td>0.036244</td>\n",
       "      <td>0.169189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          NOUN      CONJ         X       ADJ       DET       ADP       ADV  \\\n",
       "NOUN  0.264632  0.041936  0.029136  0.012289  0.013310  0.176275  0.016884   \n",
       "CONJ  0.349132  0.000469  0.007977  0.116847  0.121539  0.054435  0.055842   \n",
       "X     0.061345  0.010357  0.076482  0.016571  0.055131  0.142925  0.025175   \n",
       "ADJ   0.696725  0.016949  0.021392  0.066645  0.005101  0.078986  0.004608   \n",
       "DET   0.640029  0.000483  0.045509  0.204973  0.005311  0.009054  0.012313   \n",
       "ADP   0.321776  0.000856  0.034029  0.105297  0.326378  0.017228  0.013162   \n",
       "ADV   0.030897  0.006312  0.023588  0.130233  0.067110  0.119601  0.081063   \n",
       "PRT   0.242563  0.002288  0.013403  0.086303  0.100360  0.021576  0.010134   \n",
       "NUM   0.355338  0.013381  0.206661  0.034196  0.003271  0.034790  0.002974   \n",
       ".     0.223152  0.057924  0.026623  0.044972  0.173502  0.091114  0.052078   \n",
       "PRON  0.211230  0.004966  0.092819  0.074866  0.009549  0.022918  0.033995   \n",
       "VERB  0.110904  0.005588  0.218005  0.064649  0.133101  0.090493  0.082577   \n",
       "\n",
       "           PRT       NUM         .      PRON      VERB  \n",
       "NOUN  0.043832  0.009627  0.239179  0.004923  0.147978  \n",
       "CONJ  0.004693  0.042234  0.034256  0.058658  0.153918  \n",
       "X     0.185787  0.002709  0.163799  0.056087  0.203633  \n",
       "ADJ   0.010861  0.020405  0.065328  0.000658  0.012342  \n",
       "DET   0.000241  0.021970  0.017986  0.003742  0.038387  \n",
       "ADP   0.001498  0.062921  0.039486  0.069128  0.008240  \n",
       "ADV   0.013621  0.030565  0.136877  0.015615  0.344518  \n",
       "PRT   0.001635  0.058516  0.041517  0.018960  0.402746  \n",
       "NUM   0.027951  0.184062  0.118347  0.001487  0.017544  \n",
       ".     0.002339  0.080500  0.093812  0.065389  0.088505  \n",
       "PRON  0.012223  0.007257  0.041253  0.008021  0.480901  \n",
       "VERB  0.031121  0.022817  0.035312  0.036244  0.169189  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for vanilla viterbi algo\n",
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the vanilla viterbi algo on the validation set\n",
    "# Let's test our Viterbi algorithm on a few sample sentences of test dataset\n",
    "random.seed(1234)\n",
    "\n",
    "# choose random 5 sents\n",
    "rand_int = [random.randint(1,len(valid_set)) for x in range(5)]\n",
    "\n",
    "# list of sents\n",
    "test_run = [valid_set[i] for i in rand_int]\n",
    "\n",
    "# list of tagged words\n",
    "test_run_base = [tup for sent in test_run for tup in sent]\n",
    "\n",
    "# list of untagged words\n",
    "test_tagged_words = [tup[0] for sent in test_run for tup in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken(seconds): 14.586943864822388\n",
      "Vanilla Viterbi Algorithm Accuracy: 92.78350515463917\n"
     ]
    }
   ],
   "source": [
    "# tagging the validation sentences\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken(seconds): %s\" %(difference))\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
    "\n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "print('Vanilla Viterbi Algorithm Accuracy: %s' %(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('develops', 'NOUN'), ('develops', 'VERB')),\n",
       " (('markets', 'NOUN'), ('markets', 'VERB')),\n",
       " (('low-cost', 'NOUN'), ('low-cost', 'ADJ')),\n",
       " (('pre-1917', 'NOUN'), ('pre-1917', 'ADJ')),\n",
       " (('test', 'VERB'), ('test', 'NOUN')),\n",
       " (('out', 'PRT'), ('out', 'ADV')),\n",
       " (('copied', 'NOUN'), ('copied', 'VERB'))]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# incorrectly tagged words \n",
    "[j for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0] != j[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the problem of unknown words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see most of the unknown words are tagged as Noun. and Noun is the first tag in the taglist and is assigned if unknown words\n",
    "is identified. since emission probability of the unkown word is 0\n",
    "\n",
    "First solution to unknown words \n",
    "* <b>consider the transmission probability in case the unknown words is encounterd</b> as for the unknown words the emission probability \n",
    "is 0 so the multiplication of emission probability and transmission probability is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mofdifing the vanilla viterbi algo to consider the transmission probability if the emission probablity is 0\n",
    "def Viterbi_Mod1(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        transmission_prob =[] # storing transisition probabilities\n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            transmission_prob.append(transition_p)\n",
    "        pmax = max(p)\n",
    "        state_max = T[p.index(pmax)] \n",
    "      \n",
    "        # if probability is zero ie. there is unknown word then consider the transmission probability\n",
    "        if(pmax==0):\n",
    "            pmax = max(transmission_prob)\n",
    "            state_max = T[transmission_prob.index(pmax)]          \n",
    "        else:\n",
    "            state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  15.109654664993286\n",
      "Modified Viterbi_Mod1 Accuracy:  93.81443298969072\n"
     ]
    }
   ],
   "source": [
    "# accuracy using the modification 1 done to viterbi algo\n",
    "start = time.time()\n",
    "tagged_seq_1 = Viterbi_Mod1(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken in seconds: \", difference)\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq_1, test_run_base) if i == j] \n",
    "accuracy = len(check)/len(tagged_seq_1)\n",
    "print('Modified Viterbi_Mod1 Accuracy: ',accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modification -2\n",
    "  * <b>using a rule based tagger to backoff in case a unknown word is identified</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    (r'.*ed$', 'VERB'),               # past tense\n",
    "    (r'.*ty$', 'ADJ'),                # Adjective\n",
    "    (r'.*ing$', 'VERB'),              # gerund\n",
    "    (r'.*less$', 'ADV'),              # Adverb\n",
    "    (r'.*es$', 'VERB'),               # verb    \n",
    "    (r'.*\\'s$', 'NOUN'),              # possessive nouns\n",
    "    (r'.*s$', 'NOUN'),                # plural nouns\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'NUM'), # cardinal numbers\n",
    "    (r'.*', 'NOUN'),                  # nouns\n",
    "    (r'.*-130', 'X'),                 # nouns\n",
    "]\n",
    "\n",
    "# rule based tagger\n",
    "rule_based_tagger = nltk.RegexpTagger(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viterbi_Mod2(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        state_max = rule_based_tagger.tag([word])[0][1]       \n",
    "       \n",
    "        if(pmax==0):\n",
    "            state_max = rule_based_tagger.tag([word])[0][1] # assign based on rule based tagger\n",
    "        else:\n",
    "            if state_max != 'X':\n",
    "                # getting state for which probability is maximum\n",
    "                state_max = T[p.index(pmax)]                \n",
    "            \n",
    "        \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  15.819526433944702\n",
      "Modified Viterbi_2 Accuracy:  93.81443298969072\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "tagged_seq_2 = Viterbi_Mod2(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken in seconds: \", difference)\n",
    "check = [i for i, j in zip(tagged_seq_2, test_run_base) if i == j] \n",
    "accuracy = len(check)/len(tagged_seq_2)\n",
    "print('Modified Viterbi_2 Accuracy: ',accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('develops', 'NOUN'), ('develops', 'VERB')),\n",
       " (('markets', 'NOUN'), ('markets', 'VERB')),\n",
       " (('low-cost', 'NOUN'), ('low-cost', 'ADJ')),\n",
       " (('pre-1917', 'NOUN'), ('pre-1917', 'ADJ')),\n",
       " (('test', 'VERB'), ('test', 'NOUN')),\n",
       " (('out', 'PRT'), ('out', 'ADV'))]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# incorrectly tagged words \n",
    "[j for i, j in enumerate(zip(tagged_seq_2, test_run_base)) if j[0] != j[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating tagging accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Test_sentences.txt')\n",
    "text = f.read()\n",
    "sample_test_sent = text.splitlines()\n",
    "f.close()\n",
    "sample_test_words = [word for sent in sample_test_sent for word in sent.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  29.156334161758423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Android', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('mobile', 'ADJ'),\n",
       " ('operating', 'NOUN'),\n",
       " ('system', 'NOUN'),\n",
       " ('developed', 'VERB'),\n",
       " ('by', 'ADP'),\n",
       " ('Google.', 'NOUN'),\n",
       " ('Android', 'NOUN'),\n",
       " ('has', 'VERB'),\n",
       " ('been', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('best-selling', 'ADJ'),\n",
       " ('OS', 'NOUN'),\n",
       " ('worldwide', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('smartphones', 'NOUN'),\n",
       " ('since', 'ADP'),\n",
       " ('2011', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('on', 'ADP'),\n",
       " ('tablets', 'NOUN'),\n",
       " ('since', 'ADP'),\n",
       " ('2013.', 'NOUN'),\n",
       " ('Google', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('Twitter', 'NOUN'),\n",
       " ('made', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('deal', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('2015', 'NOUN'),\n",
       " ('that', 'ADP'),\n",
       " ('gave', 'VERB'),\n",
       " ('Google', 'NOUN'),\n",
       " ('access', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " (\"Twitter's\", 'NOUN'),\n",
       " ('firehose.', 'NOUN'),\n",
       " ('Twitter', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('an', 'DET'),\n",
       " ('online', 'NOUN'),\n",
       " ('news', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('social', 'ADJ'),\n",
       " ('networking', 'NOUN'),\n",
       " ('service', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('which', 'DET'),\n",
       " ('users', 'NOUN'),\n",
       " ('post', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('interact', 'NOUN'),\n",
       " ('with', 'ADP'),\n",
       " ('messages', 'NOUN'),\n",
       " ('known', 'VERB'),\n",
       " ('as', 'ADP'),\n",
       " ('tweets.', 'NOUN'),\n",
       " ('Before', 'ADP'),\n",
       " ('entering', 'VERB'),\n",
       " ('politics,', 'NOUN'),\n",
       " ('Donald', 'NOUN'),\n",
       " ('Trump', 'NOUN'),\n",
       " ('was', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('domineering', 'NOUN'),\n",
       " ('businessman', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('a', 'DET'),\n",
       " ('television', 'NOUN'),\n",
       " ('personality.', 'NOUN'),\n",
       " ('The', 'DET'),\n",
       " ('2018', 'NOUN'),\n",
       " ('FIFA', 'NOUN'),\n",
       " ('World', 'NOUN'),\n",
       " ('Cup', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('21st', 'NOUN'),\n",
       " ('FIFA', 'NOUN'),\n",
       " ('World', 'NOUN'),\n",
       " ('Cup,', 'NOUN'),\n",
       " ('an', 'DET'),\n",
       " ('international', 'ADJ'),\n",
       " ('football', 'NOUN'),\n",
       " ('tournament', 'NOUN'),\n",
       " ('contested', 'NOUN'),\n",
       " ('once', 'ADV'),\n",
       " ('every', 'DET'),\n",
       " ('four', 'NUM'),\n",
       " ('years.', 'NOUN'),\n",
       " ('This', 'DET'),\n",
       " ('is', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('first', 'ADJ'),\n",
       " ('World', 'NOUN'),\n",
       " ('Cup', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('be', 'VERB'),\n",
       " ('held', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('Eastern', 'NOUN'),\n",
       " ('Europe', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('the', 'DET'),\n",
       " ('11th', 'ADJ'),\n",
       " ('time', 'NOUN'),\n",
       " ('that', 'ADP'),\n",
       " ('it', 'PRON'),\n",
       " ('has', 'VERB'),\n",
       " ('been', 'VERB'),\n",
       " ('held', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('Europe.', 'NOUN'),\n",
       " ('Show', 'NOUN'),\n",
       " ('me', 'PRON'),\n",
       " ('the', 'DET'),\n",
       " ('cheapest', 'ADJ'),\n",
       " ('round', 'NOUN'),\n",
       " ('trips', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('Dallas', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('Atlanta', 'NOUN'),\n",
       " ('I', 'PRON'),\n",
       " ('would', 'VERB'),\n",
       " ('like', 'ADP'),\n",
       " ('to', 'PRT'),\n",
       " ('see', 'VERB'),\n",
       " ('flights', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('Denver', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('Philadelphia.', 'NOUN'),\n",
       " ('Show', 'NOUN'),\n",
       " ('me', 'PRON'),\n",
       " ('the', 'DET'),\n",
       " ('price', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('flights', 'NOUN'),\n",
       " ('leaving', 'VERB'),\n",
       " ('Atlanta', 'NOUN'),\n",
       " ('at', 'ADP'),\n",
       " ('about', 'ADP'),\n",
       " ('3', 'NUM'),\n",
       " ('in', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('afternoon', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('arriving', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('San', 'NOUN'),\n",
       " ('Francisco.', 'NOUN'),\n",
       " ('NASA', 'NOUN'),\n",
       " ('invited', 'NOUN'),\n",
       " ('social', 'ADJ'),\n",
       " ('media', 'NOUN'),\n",
       " ('users', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('experience', 'NOUN'),\n",
       " ('the', 'DET'),\n",
       " ('launch', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('ICESAT-2', 'NOUN'),\n",
       " ('Satellite.', 'NOUN')]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "sample_tagged_seq = Viterbi(sample_test_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken in seconds: \", difference)\n",
    "sample_tagged_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  27.664135694503784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Android', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('mobile', 'ADJ'),\n",
       " ('operating', 'NOUN'),\n",
       " ('system', 'NOUN'),\n",
       " ('developed', 'VERB'),\n",
       " ('by', 'ADP'),\n",
       " ('Google.', 'NOUN'),\n",
       " ('Android', 'NOUN'),\n",
       " ('has', 'VERB'),\n",
       " ('been', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('best-selling', 'ADJ'),\n",
       " ('OS', 'NOUN'),\n",
       " ('worldwide', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('smartphones', 'VERB'),\n",
       " ('since', 'ADP'),\n",
       " ('2011', 'NUM'),\n",
       " ('and', 'CONJ'),\n",
       " ('on', 'ADP'),\n",
       " ('tablets', 'NOUN'),\n",
       " ('since', 'ADP'),\n",
       " ('2013.', 'NOUN'),\n",
       " ('Google', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('Twitter', 'NOUN'),\n",
       " ('made', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('deal', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('2015', 'NUM'),\n",
       " ('that', 'ADP'),\n",
       " ('gave', 'VERB'),\n",
       " ('Google', 'NOUN'),\n",
       " ('access', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " (\"Twitter's\", 'NOUN'),\n",
       " ('firehose.', 'NOUN'),\n",
       " ('Twitter', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('an', 'DET'),\n",
       " ('online', 'NOUN'),\n",
       " ('news', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('social', 'ADJ'),\n",
       " ('networking', 'NOUN'),\n",
       " ('service', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('which', 'DET'),\n",
       " ('users', 'NOUN'),\n",
       " ('post', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('interact', 'NOUN'),\n",
       " ('with', 'ADP'),\n",
       " ('messages', 'VERB'),\n",
       " ('known', 'VERB'),\n",
       " ('as', 'ADP'),\n",
       " ('tweets.', 'NOUN'),\n",
       " ('Before', 'ADP'),\n",
       " ('entering', 'VERB'),\n",
       " ('politics,', 'NOUN'),\n",
       " ('Donald', 'NOUN'),\n",
       " ('Trump', 'NOUN'),\n",
       " ('was', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('domineering', 'VERB'),\n",
       " ('businessman', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('a', 'DET'),\n",
       " ('television', 'NOUN'),\n",
       " ('personality.', 'NOUN'),\n",
       " ('The', 'DET'),\n",
       " ('2018', 'NUM'),\n",
       " ('FIFA', 'NOUN'),\n",
       " ('World', 'NOUN'),\n",
       " ('Cup', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('21st', 'NOUN'),\n",
       " ('FIFA', 'NOUN'),\n",
       " ('World', 'NOUN'),\n",
       " ('Cup,', 'NOUN'),\n",
       " ('an', 'DET'),\n",
       " ('international', 'ADJ'),\n",
       " ('football', 'NOUN'),\n",
       " ('tournament', 'NOUN'),\n",
       " ('contested', 'VERB'),\n",
       " ('once', 'ADV'),\n",
       " ('every', 'DET'),\n",
       " ('four', 'NUM'),\n",
       " ('years.', 'NOUN'),\n",
       " ('This', 'DET'),\n",
       " ('is', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('first', 'ADJ'),\n",
       " ('World', 'NOUN'),\n",
       " ('Cup', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('be', 'VERB'),\n",
       " ('held', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('Eastern', 'NOUN'),\n",
       " ('Europe', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('the', 'DET'),\n",
       " ('11th', 'ADJ'),\n",
       " ('time', 'NOUN'),\n",
       " ('that', 'ADP'),\n",
       " ('it', 'PRON'),\n",
       " ('has', 'VERB'),\n",
       " ('been', 'VERB'),\n",
       " ('held', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('Europe.', 'NOUN'),\n",
       " ('Show', 'NOUN'),\n",
       " ('me', 'PRON'),\n",
       " ('the', 'DET'),\n",
       " ('cheapest', 'ADJ'),\n",
       " ('round', 'NOUN'),\n",
       " ('trips', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('Dallas', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('Atlanta', 'NOUN'),\n",
       " ('I', 'PRON'),\n",
       " ('would', 'VERB'),\n",
       " ('like', 'ADP'),\n",
       " ('to', 'PRT'),\n",
       " ('see', 'VERB'),\n",
       " ('flights', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('Denver', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('Philadelphia.', 'NOUN'),\n",
       " ('Show', 'NOUN'),\n",
       " ('me', 'PRON'),\n",
       " ('the', 'DET'),\n",
       " ('price', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('flights', 'NOUN'),\n",
       " ('leaving', 'VERB'),\n",
       " ('Atlanta', 'NOUN'),\n",
       " ('at', 'ADP'),\n",
       " ('about', 'ADP'),\n",
       " ('3', 'NUM'),\n",
       " ('in', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('afternoon', 'NOUN'),\n",
       " ('and', 'CONJ'),\n",
       " ('arriving', 'VERB'),\n",
       " ('in', 'ADP'),\n",
       " ('San', 'NOUN'),\n",
       " ('Francisco.', 'NOUN'),\n",
       " ('NASA', 'NOUN'),\n",
       " ('invited', 'VERB'),\n",
       " ('social', 'ADJ'),\n",
       " ('media', 'NOUN'),\n",
       " ('users', 'NOUN'),\n",
       " ('to', 'PRT'),\n",
       " ('experience', 'NOUN'),\n",
       " ('the', 'DET'),\n",
       " ('launch', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('ICESAT-2', 'NOUN'),\n",
       " ('Satellite.', 'NOUN')]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "sample_tagged_seq = Viterbi_Mod2(sample_test_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken in seconds: \", difference)\n",
    "sample_tagged_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of vanilla Viterbi Algorithm: 92 %\n",
    "\n",
    "The accuracy of modified Viterbi with rule base backoffad Algorithm: 94%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List down cases which were incorrectly tagged by original POS tagger and got corrected by your modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cases were incorrectly tagged which got corrected by modified Viterbi Algorithm:\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Word</th>\n",
    "            <th>Wrong POS tag</th>\n",
    "            <th>Corrected POS tag</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Nonetheless</td>\n",
    "            <td>Noun</td>\n",
    "            <td>Adv</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>plenty</td>\n",
    "            <td>Noun</td>\n",
    "            <td>ADJ</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>lofty</td>\n",
    "            <td>Noun</td>\n",
    "            <td>ADJ</td>\n",
    "        </tr>\n",
    "    </table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
